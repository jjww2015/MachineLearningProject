rfresults
confusionMatrix(testing$diagnosis, predict(rf, testing))$overall["Accuracy"]
confusionMatrix(testing$diagnosis, predict(gbm, testing))$overall["Accuracy"]
confusionMatrix(testing$diagnosis, predict(lda, testing))$overall["Accuracy"]
confusionMatrix(testing$diagnosis, predict(comfit, testing))$overall["Accuracy"]
newdatatest <- data.frame(predict(rf, testing), predict(gbm, testing), predict(lda, testing), testing$diagnosis)
confusionMatrix(testing$diagnosis, predict(comfit, newdatatest))$overall["Accuracy"]
predict(comfit, newdatatest)
getwd()
confusionMatrix(testing$diagnosis, rfresults)$overall["Accuracy"]
confusionMatrix(testing$diagnosis, gbmresults)$overall["Accuracy"]
confusionMatrix(testing$diagnosis, ldaresults)$overall["Accuracy"]
confusionMatrix(testing$diagnosis, predict(rf, testing))$overall["Accuracy"]
confusionMatrix(testing$diagnosis, predict(gbm, testing))$overall["Accuracy"]
confusionMatrix(testing$diagnosis, predict(lda, testing))$overall["Accuracy"]
confusionMatrix(testing$diagnosis, predict(comfit, newtesting))$overall["Accuracy"]
source('~/work/datasciencecoursera/data/quiz4_Q2.R', echo=TRUE)
source('~/work/datasciencecoursera/data/quiz4_Q2.R', echo=TRUE)
newtesting <- data.frame(predict(rf, testing), predict(gbm, testing), predict(lda, test), diagnosis = testing$diagnosis)
newtesting <- data.frame(predict(rf, testing), predict(gbm, testing), predict(lda, testing), diagnosis = testing$diagnosis)
comfit <- train(diagnosis ~., method = "rf", data = newtraining)
confusionMatrix(testing$diagnosis, predict(rf, testing))$overall["Accuracy"]
confusionMatrix(testing$diagnosis, predict(gbm, testing))$overall["Accuracy"]
confusionMatrix(testing$diagnosis, predict(lda, testing))$overall["Accuracy"]
confusionMatrix(testing$diagnosis, predict(comfit, newtesting))$overall["Accuracy"]
confusionMatrix(testing$diagnosis, predict(comfit, testing))$overall["Accuracy"]
comfit <- train(diagnosis ~., method = "rf", data = newtraining)
comfit
predict(testing, comfit)
rfresults <- predict(rf, testing)
gbmresults <- predict(gbm, testing)
ldaresults <- predict(lda, testing)
newdata <- data.frame(rfresults, gbmresults, ldaresults, diagnosis = testing$diagnosis)
comfit <- train(diagnosis ~., method = "rf", data = newdata)
confusionMatrix(testing$diagnosis, predict(comfit, testing))$overall["Accuracy"]
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
names(training)
?plot.enet
fitlasso <- train(CompressiveStrength ~., method = "lasso", data = training)
fitlasso <- train(CompressiveStrength ~., method = "lasso", data = training)
?plot.enet
plot.enet(fitlasso)
?enet
enet(fitlasso, training$CompressiveStrength)
plot.enet(fitlasso$finalModel, xvar="penalty", use.color=TRUE)
library(lubridate)
dat = read.csv("./gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
?bats
names(training)
dim(training)
fit <- bats(training)
head(traininig)
head(training)
fit <- bats(tstrain)
fcast <-forecast(fit)
plot(fcast)
tstest <- ts(testing$visitsTumblr)
lines(tstest, col = "red")
head(tstest)
accuracy(fcast, tstest)
?plot.forecast
?forecast
fcast <- forecast.bats(fit, level=95, h=nrow(testing))
acc <- accuracy(fcast, testing$visitsTumblr)
?forecast.bats
nrow(testing)
autoplot(fcast)
fcast$lower
tmp <-data.frame(lower = fcast$lower, upper = fcast$upper, vT = testing$visitsTumblr)
tmp <- testing$visitsTumblr >fcast$lower & testing$visitsTumblr <fcast$upper
dim(tmp)
dim(testing)
mean(tmp)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
?svm
fit <- svm(CompressiveStrength ~., data = training)
?svm
svm
install.packages("e1071", dep = T)
install.packages("e1071", dep = T)
fit <- svm(CompressiveStrength ~., data = training)
library('e1071')
fit <- svm(CompressiveStrength ~., data = training)
tmp <-predict(fit, testing)
dim(tmp)
length(tmp)
sqrt(sum(tmp^2)/length(tmp))
fit
pre <- predict(fit, testing)
sqrt(sum((pre-testing$CompressiveStrength)^2)/length(tmp))
q()
install.packages(shiny)
install.packages("shiny", dep = T)
library(shiny)
install.packages("rChart", dep = T)
runApp('~/test')
runApp('~/work/DS/09_DevelopingDataProducts/shiny/simplestApp')
runApp('~/test')
runApp('~/work/DS/09_DevelopingDataProducts/shiny/markupApp')
runApp('~/work/DS/09_DevelopingDataProducts/shiny/markupApp')
runApp('~/work/DS/09_DevelopingDataProducts/shiny/markupApp')
runApp('~/work/DS/09_DevelopingDataProducts/shiny/inputApp')
q()
q()
- Bullet 1
getwd()
package.skeleton()
training <-read.csv("pml-training.csv")
namees(training)
names(training)
head(training[,160])
library(caret)
nzv <- nearZeroVar(training, saveMetrics = TRUE)
dim(nzv)
head(nzv)
sum(nzv[,nzv])
sum(nzv[,"nzv"])
nzv[nzv[,"nzv"]==TRUE,]
hist(training[,"new_window"])
head(training[,"new_window"])
head(training[,"kurtosis_roll_belt"])
hist(training[,"kurtosis_roll_belt"])
hist(as.numeric(training[,"kurtosis_roll_belt"]))
hist(log10(as.numeric(training[,"kurtosis_roll_belt"])+1))
dim(training)
head(training[,"username"])
head(training[,"user_name"])
head(training[,"classe"])
training <- read.csv("pml-training.csv", stringsAsFactors = FALSE)
nzv <- nearZeroVar(training, saveMetrics = TRUE)
sum(nzv[,"nzv"])
head(nzv)
nzv[nzv[,"nzv"]==TRUE,]
hist(training[,"new_window"])
str(training)
summary(training[,"user_name"])
levels(training[,"user_name"])
facctors(raining[,"user_name"])
factors(raining[,"user_name"])
factor(raining[,"user_name"])
factor(training[,"user_name"])
names(training)
summary(training[,'new_window'])
head(training[,'new_window'])
head(training[,'number_window'])
head(training[,'num_window'])
summary(training[,'num_window'])
plot(training[,'num_window'])
hist(training[,'num_window'])
hist(training[,'classe'])
hist(as.numeric(training[,'classe']))
hist(factor(training[,'classe'])_
hist(factor(training[,'classe']))
tabel(factor(training[,'classe']))
tabl(feactor(training[,'classe']))
tableee(feactor(training[,'classe']))
table(feactor(training[,'classe']))
table(factor(training[,'classe']))
att <- names(training)
View(att)
head(att)
head(att,20)
str(training[,"X"])
hist(training[,"X"])
plot(training[,"X"])
att
str(training[,1:10])
q()
trainURL <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
train_DF <- read.csv(file=trainURL, na.strings = c("NA", ""), stringsAsFactors=FALSE)
str(train_DF)
# Load training data
# download from "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
training <- read.csv(file = "pml-training.csv",
na.strings=c("NA","NaN", " ", "", "#DIV/0!"))
# load test data
# download from "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
testing <- read.csv(file = "pml-testing.csv",
na.strings=c("NA","NaN", " ", "", "#DIV/0!"))
sapply(training, is.numeric)
str(training)
?cor
correlations <- cor(training[sapply(training, is.numeric)])
corrplot.mixed(correlations, order = "hclust", tl.col="black", diag = "n", tl.pos = "lt",
lower = "circle", upper = "number", tl.cex = 1.5, mar=c(1,0,1,0))
library('corrplot')
install.packages("corrplot", dep = T)
library(corrplot)
par(ps=5)
corrplot.mixed(correlations, order = "hclust", tl.col="black", diag = "n", tl.pos = "lt",
lower = "circle", upper = "number", tl.cex = 1.5, mar=c(1,0,1,0))
q()
apropos("varimpplot")
apropos("varimp")
apropos("var")
q(0)
q()
?train
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(corrplot)
library(doMC)
registerDoMC(cores = 8) # my PC have 8 cores
# Load training data
# download from "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
data <- read.csv(file = "pml-training.csv",
na.strings=c("NA","NaN", " ", "", "#DIV/0!"))
# load test data
# download from "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
test20 <- read.csv(file = "pml-testing.csv",
na.strings=c("NA","NaN", " ", "", "#DIV/0!"))
# set random seed
set.seed(12234)
# split dataframe into training and testing sets
# 60% of total data goes towards training and 40% of the remaining for testing
inTrain <- createDataPartition(y = data$classe, p=0.60, list=FALSE)
training <- data[inTrain, ]
testing <- data[-inTrain, ]
# remove user name and other identify info
metainfo <- training[,1:7]
nzv <- nearZeroVar(training[,-c(1:7)])
# remove near zero variance features
trainExtract <- training[,-c(nzv,1:7)]
# remove features with missing values
trainExtract <- trainExtract[colSums(is.na(trainExtract)) == 0]
# remove features high correlation (>0.75)
correlations <- cor(trainExtract[,-length(trainExtract)])
corrplot.mixed(correlations, order = "hclust", tl.col="black",
tl.pos = "lt", lower = "circle", upper = "number")
highCorr <- findCorrelation(correlations, cutoff = .75)
print(highCorr)
trainExtract <- trainExtract[, -highCorr]
# The predictors used
names(trainExtract)
?train
str(trainExtract)
set.seed(12234)
treebagFit <- train(classe~., data=trainExtract, method="treebag", trControl=trainControl(method = "cv", number = 4))
varImpPlot(treebagFit$finalModel, sort = TRUE, type = 1, pch = 19, col = 1, cex = 1,
main = sprintf("Relative Importance of Top %d Predictor Variables",nrow(treebagFit$finalModel$importance)))
search()
rfFit <- train(classe ~., method = "rf", data = trainExtract,
trControl = trainControl(method = "cv", number = 4),
ntree = 100, importance = TRUE)
varImpPlot(rfFit$finalModel, sort = TRUE, type = 1, pch = 19, col = 1, cex = 1,
main = sprintf("Relative Importance of Top %d Predictor Variables",nrow(rfFit$finalModel$importance)))
bagging_results <- resamples(list(treebag=treebagFit, rf=rfFit))
summary(bagging_results)
dotplot(bagging_results)
treebagFit
?trainControl
# apply Random Forest model to validation set
rfFit_validate <- predict(rfFit, testing)
# generate confusion matrix for validation model
CM <- confusionMatrix(testing$classe, rfFit_validate)
CM$table
# apply Random Forest model to validation set
rfFit_validate <- predict(rfFit, testing)
# generate confusion matrix for validation model
CM1 <- confusionMatrix(testing$classe, rfFit_validate)
CM1$table
treebagFit_validate <- predict(treebagFit, testing)
# generate confusion matrix for validation model
CM2 <- confusionMatrix(testing$classe, treebagFit_validate)
CM2$table
CM1$overall
CM2$overall
tmp <- predict(bagging_results, testing)
# Load training data
# download from "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
data <- read.csv(file = "pml-training.csv",
na.strings=c("NA","NaN", " ", "", "#DIV/0!"))
# load test data
# download from "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
test20 <- read.csv(file = "pml-testing.csv",
na.strings=c("NA","NaN", " ", "", "#DIV/0!"))
# set random seed
set.seed(12234)
# split dataframe into training and testing sets
# 60% of total data goes towards training and 40% of the remaining for testing
inTrain <- createDataPartition(y = data$classe, p=0.60, list=FALSE)
training <- data[inTrain, ]
testing <- data[-inTrain, ]
# remove user name and other identify info
metainfo <- training[,1:7]
nzv <- nearZeroVar(training[,-c(1:7)])
# remove near zero variance features
trainExtract <- training[,-c(nzv,1:7)]
# remove features with missing values
trainExtract <- trainExtract[colSums(is.na(trainExtract)) == 0]
# remove features high correlation (>0.75)
par(ps=6)
correlations <- cor(trainExtract[,-length(trainExtract)])
corrplot.mixed(correlations, order = "hclust", tl.col="black",
tl.pos = "lt", lower = "circle", upper = "number")
highCorr <- findCorrelation(correlations, cutoff = .75)
print(highCorr)
trainExtractNoCorr <- trainExtract[, -highCorr]
# The predictors used
names(trainExtract)
#control <- trainControl(method="repeatedcv", number=5, repeats=3)
# Random Forest
set.seed(12234)
rfFit_withHighCor <- train(classe ~., method = "rf", data = trainExtract,
trControl = trainControl(method = "repeatedcv", number = 4),
ntree = 100, importance = TRUE)
# visualize the relative importance of each predictor variable in the model
varImpPlot(rfFit_withHighCor$finalModel, sort = TRUE, type = 1, pch = 19, col = 1, cex = 1,
main = sprintf("Relative Importance of %d Predictor Variables",nrow(rfFit_withHighCor$finalModel$importance)))
# Random Forest
set.seed(12234)
rfFit_noHighCor <- train(classe ~., method = "rf", data = trainExtractNoCorr,
trControl = trainControl(method = "repeatedcv", number = 4),
ntree = 100, importance = TRUE)
# visualize the relative importance of each predictor variable in the model
varImpPlot(rfFit_noHighCor$finalModel, sort = TRUE, type = 1, pch = 19, col = 1, cex = 1,
main = sprintf("Relative Importance of %d Predictor Variables",nrow(rfFit_noHighCor$finalModel$importance)))
# summarize results
resamp <- resamples(list(HighCor=rfFit_withHighCor, noHighCor=rfFit_withHighCor))
summary(resamp)
dotplot(resamp)
# apply Random Forest model to validation set
rfFit_validate1 <- predict(rfFit_withHighCor, testing)
# generate confusion matrix for validation model
CM1 <- confusionMatrix(testing$classe, rfFit_validate1)
CM1$table
CM1$overall
rfFit_validate2 <- predict(rfFit_noHighCor, testing)
# generate confusion matrix for validation model
CM2 <- confusionMatrix(testing$classe, rfFit_validate2)
CM2$table
CM2$overall
#control <- trainControl(method="repeatedcv", number=5, repeats=3)
# Random Forest
set.seed(12234)
rfFit_withHighCor <- train(classe ~., method = "rf", data = trainExtract,
trControl = trainControl(method = "repeatedcv", number = 6, repeat = 2),
ntree = 100, importance = TRUE)
# visualize the relative importance of each predictor variable in the model
varImpPlot(rfFit_withHighCor$finalModel, sort = TRUE, type = 1, pch = 19, col = 1, cex = 1,
main = sprintf("Relative Importance of %d Predictor Variables",nrow(rfFit_withHighCor$finalModel$importance)))
# Random Forest
set.seed(12234)
rfFit_noHighCor <- train(classe ~., method = "rf", data = trainExtractNoCorr,
trControl = trainControl(method = "repeatedcv", number = 6, repeat = 2),
ntree = 100, importance = TRUE)
# visualize the relative importance of each predictor variable in the model
varImpPlot(rfFit_noHighCor$finalModel, sort = TRUE, type = 1, pch = 19, col = 1, cex = 1,
main = sprintf("Relative Importance of %d Predictor Variables",nrow(rfFit_noHighCor$finalModel$importance)))
# summarize results
resamp <- resamples(list(HighCor=rfFit_withHighCor, noHighCor=rfFit_withHighCor))
summary(resamp)
dotplot(resamp)
#control <- trainControl(method="repeatedcv", number=5, repeats=3)
# Random Forest
set.seed(12234)
rfFit_withHighCor <- train(classe ~., method = "rf", data = trainExtract,
trControl = trainControl(method = "repeatedcv", number = 6, repeats = 2),
ntree = 100, importance = TRUE)
# visualize the relative importance of each predictor variable in the model
varImpPlot(rfFit_withHighCor$finalModel, sort = TRUE, type = 1, pch = 19, col = 1, cex = 1,
main = sprintf("Relative Importance of %d Predictor Variables",nrow(rfFit_withHighCor$finalModel$importance)))
# Random Forest
set.seed(12234)
rfFit_noHighCor <- train(classe ~., method = "rf", data = trainExtractNoCorr,
trControl = trainControl(method = "repeatedcv", number = 6, repeats = 2),
ntree = 100, importance = TRUE)
# visualize the relative importance of each predictor variable in the model
varImpPlot(rfFit_noHighCor$finalModel, sort = TRUE, type = 1, pch = 19, col = 1, cex = 1,
main = sprintf("Relative Importance of %d Predictor Variables",nrow(rfFit_noHighCor$finalModel$importance)))
# summarize results
resamp <- resamples(list(HighCor=rfFit_withHighCor, noHighCor=rfFit_withHighCor))
summary(resamp)
dotplot(resamp)
# apply Random Forest model to validation set
rfFit_validate1 <- predict(rfFit_withHighCor, testing)
# generate confusion matrix for validation model
CM1 <- confusionMatrix(testing$classe, rfFit_validate1)
CM1$table
CM1$overall
rfFit_validate2 <- predict(rfFit_noHighCor, testing)
# generate confusion matrix for validation model
CM2 <- confusionMatrix(testing$classe, rfFit_validate2)
CM2$table
CM2$overall
#control <- trainControl(method="repeatedcv", number=5, repeats=3)
# Random Forest
set.seed(12234)
rfFit_withHighCor <- train(classe ~., method = "rf", data = trainExtract,
trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10),
ntree = 10, importance = TRUE)
# visualize the relative importance of each predictor variable in the model
varImpPlot(rfFit_withHighCor$finalModel, sort = TRUE, type = 1, pch = 19, col = 1, cex = 1,
main = sprintf("Relative Importance of %d Predictor Variables",nrow(rfFit_withHighCor$finalModel$importance)))
# Random Forest
set.seed(12234)
rfFit_noHighCor <- train(classe ~., method = "rf", data = trainExtractNoCorr,
trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10),
ntree = 10, importance = TRUE)
# visualize the relative importance of each predictor variable in the model
varImpPlot(rfFit_noHighCor$finalModel, sort = TRUE, type = 1, pch = 19, col = 1, cex = 1,
main = sprintf("Relative Importance of %d Predictor Variables",nrow(rfFit_noHighCor$finalModel$importance)))
# summarize results
resamp <- resamples(list(HighCor=rfFit_withHighCor, noHighCor=rfFit_withHighCor))
summary(resamp)
dotplot(resamp)
# apply Random Forest model to validation set
rfFit_validate1 <- predict(rfFit_withHighCor, testing)
# generate confusion matrix for validation model
CM1 <- confusionMatrix(testing$classe, rfFit_validate1)
CM1$table
CM1$overall
rfFit_validate2 <- predict(rfFit_noHighCor, testing)
# generate confusion matrix for validation model
CM2 <- confusionMatrix(testing$classe, rfFit_validate2)
CM2$table
CM2$overall
dim(trainExtract)
names(trainExtract)
setwd("~/work/datasciencecoursera/data/practiceMLProj")
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(corrplot)
library(doMC)
registerDoMC(cores = 8) # my PC have 8 cores
# Load training data
# download from "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
data <- read.csv(file = "pml-training.csv",
na.strings=c("NA","NaN", " ", "", "#DIV/0!"))
# load test data
# download from "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
test20 <- read.csv(file = "pml-testing.csv",
na.strings=c("NA","NaN", " ", "", "#DIV/0!"))
# set random seed
set.seed(12234)
# split dataframe into training and testing sets
# 60% of total data goes towards training and 40% of the remaining for testing
inTrain <- createDataPartition(y = data$classe, p=0.60, list=FALSE)
training <- data[inTrain, ]
testing <- data[-inTrain, ]
# remove user name and other identify info
metainfo <- training[,1:7]
nzv <- nearZeroVar(training[,-c(1:7)])
# remove near zero variance features
trainExtract <- training[,-c(nzv,1:7)]
# remove features with missing values
trainExtract <- trainExtract[colSums(is.na(trainExtract)) == 0]
# remove features high correlation (>0.75)
par(ps=6)
correlations <- cor(trainExtract[,-length(trainExtract)])
corrplot.mixed(correlations, order = "hclust", tl.col="black",
tl.pos = "lt", lower = "circle", upper = "number")
highCorr <- findCorrelation(correlations, cutoff = .75)
print(highCorr)
trainExtractNoCorr <- trainExtract[, -highCorr]
# The predictors used without removing high correlated features
names(trainExtract)
# The high correlated features
names(trainExtract)[highCorr]
#control <- trainControl(method="repeatedcv", number=5, repeats=3)
# Random Forest
set.seed(12234)
rfFit_withHighCor <- train(classe ~., method = "rf", data = trainExtract,
trControl = trainControl(method = "cv", number = 4),
ntree = 100, importance = TRUE)
# visualize the relative importance of each predictor variable in the model
varImpPlot(rfFit_withHighCor$finalModel, sort = TRUE, type = 1, pch = 19, col = 1, cex = 1,
main = sprintf("Relative Importance of %d Predictor Variables \nwith correlated features",nrow(rfFit_withHighCor$finalModel$importance)))
# Random Forest
set.seed(12234)
rfFit_noHighCor <- train(classe ~., method = "rf", data = trainExtractNoCorr,
trControl = trainControl(method = "cv", number = 4),
ntree = 100, importance = TRUE)
# visualize the relative importance of each predictor variable in the model
varImpPlot(rfFit_noHighCor$finalModel, sort = TRUE, type = 1, pch = 19, col = 1, cex = 1,
main = sprintf("Relative Importance of %d Predictor Variables \nwithout correlated features",nrow(rfFit_noHighCor$finalModel$importance)))
# summarize results
resamp <- resamples(list(HighCor=rfFit_withHighCor, noHighCor=rfFit_withHighCor))
summary(resamp)
dotplot(resamp)
# apply Random Forest model to validation set
rfFit_validate1 <- predict(rfFit_withHighCor, testing)
# generate confusion matrix for validation model
CM1 <- confusionMatrix(testing$classe, rfFit_validate1)
CM1$table
CM1$overall
rfFit_validate2 <- predict(rfFit_noHighCor, testing)
# generate confusion matrix for validation model
CM2 <- confusionMatrix(testing$classe, rfFit_validate2)
CM2$table
CM2$overall
test20pre <- predict(rfFit_withHighCor, test20)
print(test20pre)
CM1$overall["Accuracy"]
CM1$overall[["Accuracy"]]
1-CM1$overall[["Accuracy"]]
q()
